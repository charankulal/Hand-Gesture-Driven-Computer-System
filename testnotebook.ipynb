{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mouse actions\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import HandTrackingModule as htm\n",
    "import math\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import pyautogui\n",
    "\n",
    "wCam, hCam = 640, 480\n",
    "cap=cv2.VideoCapture(0)\n",
    "cap.set(3,wCam)\n",
    "cap.set(4,hCam)\n",
    "pTime = 0\n",
    "frameR=100\n",
    "smoothening=12\n",
    "plocX,plocY=0,0\n",
    "clocX,clocY=0,0\n",
    "\n",
    "\n",
    "detector=htm.HandDetector(maxHands=1)\n",
    "wScreen,hScreen=pyautogui.size()\n",
    "pyautogui.FAILSAFE=False\n",
    "\n",
    "while True:\n",
    "    success,img=cap.read()\n",
    "    img = cv2.flip(img, 1)\n",
    "    img=detector.findHands(img)\n",
    "    lmList,bbox=detector.findPosition(img)\n",
    "    \n",
    "    if len(lmList)!=0:\n",
    "        [x1,y1]=lmList[8][1:]\n",
    "        [x2,y2]=lmList[12][1:]\n",
    "        fingers=detector.fingersUp()\n",
    "        cv2.rectangle(img,(frameR,frameR),(wCam-frameR,hCam-frameR),(255,0,255),2)\n",
    "        \n",
    "        # For hovering\n",
    "        if fingers[1]==1 and fingers[2]==0:\n",
    "            \n",
    "            x3=np.interp(x1,(frameR,wCam-frameR),(0,wScreen))\n",
    "            y3=np.interp(y1,(frameR,hCam-frameR),(0,hScreen))\n",
    "            \n",
    "            clocX=plocX+(x3-plocX)/smoothening\n",
    "            clocY=plocY+(y3-plocY)/smoothening\n",
    "            \n",
    "            pyautogui.moveTo(clocX,clocY)\n",
    "            plocX,plocY=clocX,clocY\n",
    "        \n",
    "        # Left click functionality\n",
    "        \n",
    "        if fingers[1]==1 and fingers[2]==1:\n",
    "            length,img,lineInfo=detector.findDistance(8,12,img)\n",
    "            # print(length)\n",
    "            if length<40:\n",
    "                pyautogui.click()\n",
    "        \n",
    "        # Design for right click\n",
    "        if fingers[1]==1 and fingers[2]==0:\n",
    "            if fingers[4]==1:\n",
    "                pyautogui.rightClick()\n",
    "                \n",
    "        \n",
    "    \n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime-pTime)\n",
    "    pTime = cTime\n",
    "\n",
    "    cv2.putText(img, f'FPS:{int(fps)}', (48, 70),\n",
    "                cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 3)\n",
    "    cv2.imshow(\"Image\",img)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volume control gesture --- Use right hand for the volume control\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import HandTrackingModule as htm\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "wCam, hCam = 640, 480\n",
    "# boundingBox=[]\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, wCam)\n",
    "cap.set(4, hCam)\n",
    "pTime = 0\n",
    "\n",
    "\n",
    "detector = htm.HandDetector(maxHands=1)\n",
    "\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(\n",
    "    IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = interface.QueryInterface(IAudioEndpointVolume)\n",
    "volumeRange = volume.GetVolumeRange()\n",
    "minimumVolume = volumeRange[0]\n",
    "maximumVolume = volumeRange[1]\n",
    "area = 0\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img = cv2.flip(img, 1)  # Flipping is needed to create mirroring\n",
    "    # Find the hands\n",
    "    img,which_hand=detector.findHands(img)\n",
    "    lmList, boundingBox = detector.findPosition(img, handNo=0, draw=True)\n",
    "    if which_hand==\"Right\":\n",
    "        if len(lmList) != 0:\n",
    "        # Filter based on size\n",
    "            area = (boundingBox[2]-boundingBox[0]) * \\\n",
    "                (boundingBox[3]-boundingBox[1])//100\n",
    "            # print(area)\n",
    "            if 100 < area < 900:\n",
    "\n",
    "                # Find the distance btwn index and thumb\n",
    "                length, img, lineInfo = detector.findDistance(4, 8, img=img)\n",
    "\n",
    "                # Converting length to volume\n",
    "                vol = np.interp(length, [10, 150], [0, 100])\n",
    "\n",
    "                # Reduce resolution to make smoother.\n",
    "                smoothness = 5\n",
    "                vol = smoothness*round(vol/smoothness)\n",
    "\n",
    "                # Check fingers which are up\n",
    "                fingers=detector.fingersUp()\n",
    "            \n",
    "\n",
    "                # if pinky is down then set volume\n",
    "                if not fingers[4]:\n",
    "                    volume.SetMasterVolumeLevelScalar(vol/100, None)\n",
    "                \n",
    "            \n",
    "\n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime-pTime)\n",
    "    pTime = cTime\n",
    "\n",
    "    cv2.putText(img, f'FPS:{int(fps)}', (48, 70),\n",
    "                cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 3)\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import HandTrackingModule as htm\n",
    "import screen_brightness_control as sbc\n",
    "\n",
    "wCam, hCam = 640, 480\n",
    "# boundingBox=[]\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, wCam)\n",
    "cap.set(4, hCam)\n",
    "pTime = 0\n",
    "\n",
    "\n",
    "detector = htm.HandDetector(maxHands=1)\n",
    "area = 0\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img = cv2.flip(img, 1)  # Flipping is needed to create mirroring\n",
    "    # Find the hands\n",
    "    img,which_hand=detector.findHands(img)\n",
    "    lmList, boundingBox = detector.findPosition(img, handNo=0, draw=True)\n",
    "    if which_hand==\"Left\":\n",
    "        if len(lmList) != 0:\n",
    "        # Filter based on size\n",
    "            area = (boundingBox[2]-boundingBox[0]) * \\\n",
    "                (boundingBox[3]-boundingBox[1])//100\n",
    "            # print(area)\n",
    "            if 100 < area < 900:\n",
    "\n",
    "                # Find the distance btwn index and thumb\n",
    "                length, img, lineInfo = detector.findDistance(4, 8, img=img)\n",
    "\n",
    "                # Converting length to volume\n",
    "                brightness = np.interp(length, [10, 150], [0, 100])\n",
    "\n",
    "                # Reduce resolution to make smoother.\n",
    "                smoothness = 5\n",
    "                brightness = smoothness*round(brightness/smoothness)\n",
    "\n",
    "                # Check fingers which are up\n",
    "                fingers=detector.fingersUp()\n",
    "            \n",
    "\n",
    "                # if pinky is down then set volume\n",
    "                if not fingers[4]:\n",
    "                    sbc.set_brightness(brightness)\n",
    "\n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime-pTime)\n",
    "    pTime = cTime\n",
    "\n",
    "    cv2.putText(img, f'FPS:{int(fps)}', (48, 70),\n",
    "                cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 3)\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample use only..Not working\n",
    "\n",
    "# Importing Libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy as np\n",
    "import HandTrackingModule as htm\n",
    "import math\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "# Used to convert protobuf message to a dictionary.\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "# Initializing the Model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.75,\n",
    "    min_tracking_confidence=0.75,\n",
    "    max_num_hands=2)\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "# Start capturing video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read video frame by frame\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip the image(frame)\n",
    "    img = cv2.flip(img, 1)\n",
    "\n",
    "    # Convert BGR image to RGB image\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the RGB image\n",
    "    results = hands.process(imgRGB)\n",
    "\n",
    "    # If hands are present in image(frame)\n",
    "    if results.multi_hand_landmarks:\n",
    "\n",
    "        # Both Hands are present in image(frame)\n",
    "        if len(results.multi_handedness) == 2:\n",
    "            # Display 'Both Hands' on the image\n",
    "            cv2.putText(img, 'Both Hands', (250, 50),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX,\n",
    "                        0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # If any hand present\n",
    "        else:\n",
    "            for i in results.multi_handedness:\n",
    "\n",
    "                # Return whether it is Right or Left Hand\n",
    "                label = MessageToDict(i)['classification'][0]['label']\n",
    "\n",
    "                if label == 'Left':\n",
    "\n",
    "                    # Display 'Left Hand' on\n",
    "                    # left side of window\n",
    "                    cv2.putText(img, label+' Hand',\n",
    "                                (20, 50),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                0.9, (0, 255, 0), 2)\n",
    "\n",
    "                if label == 'Right':\n",
    "\n",
    "                    # Display 'Left Hand'\n",
    "                    # on left side of window\n",
    "                    cv2.putText(img, label+' Hand', (460, 50),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                0.9, (0, 255, 0), 2)\n",
    "                    # Draw landmark points\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "    # Display Video and when 'q'\n",
    "    # is entered, destroy the window\n",
    "    cv2.imshow('Image', img)\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
