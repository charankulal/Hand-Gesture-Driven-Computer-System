{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m success,img\u001b[38;5;241m=\u001b[39mcap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     27\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mflip(img, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m img\u001b[38;5;241m=\u001b[39m\u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindHands\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m lmList,bbox\u001b[38;5;241m=\u001b[39mdetector\u001b[38;5;241m.\u001b[39mfindPosition(img)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lmList)\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\PROJECTS\\Hand Tracking\\HandTrackingModule.py:21\u001b[0m, in \u001b[0;36mHandDetector.findHands\u001b[1;34m(self, img, draw)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindHands\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, draw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     20\u001b[0m     imgRGB \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgRGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m handLms \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n",
      "File \u001b[1;32md:\\PROJECTS\\Hand Tracking\\.venv\\Lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PROJECTS\\Hand Tracking\\.venv\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:372\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    366\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    368\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    369\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    370\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Mouse actions\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import HandTrackingModule as htm\n",
    "import math\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import pyautogui\n",
    "\n",
    "wCam, hCam = 640, 480\n",
    "cap=cv2.VideoCapture(0)\n",
    "cap.set(3,wCam)\n",
    "cap.set(4,hCam)\n",
    "pTime = 0\n",
    "frameR=100\n",
    "smoothening=12\n",
    "plocX,plocY=0,0\n",
    "clocX,clocY=0,0\n",
    "\n",
    "\n",
    "detector=htm.HandDetector(maxHands=1)\n",
    "wScreen,hScreen=pyautogui.size()\n",
    "pyautogui.FAILSAFE=False\n",
    "\n",
    "while True:\n",
    "    success,img=cap.read()\n",
    "    img = cv2.flip(img, 1)\n",
    "    img=detector.findHands(img)\n",
    "    lmList,bbox=detector.findPosition(img)\n",
    "    \n",
    "    if len(lmList)!=0:\n",
    "        [x1,y1]=lmList[8][1:]\n",
    "        [x2,y2]=lmList[12][1:]\n",
    "        fingers=detector.fingersUp()\n",
    "        cv2.rectangle(img,(frameR,frameR),(wCam-frameR,hCam-frameR),(255,0,255),2)\n",
    "        \n",
    "        # For hovering\n",
    "        if fingers[1]==1 and fingers[2]==0:\n",
    "            \n",
    "            x3=np.interp(x1,(frameR,wCam-frameR),(0,wScreen))\n",
    "            y3=np.interp(y1,(frameR,hCam-frameR),(0,hScreen))\n",
    "            \n",
    "            clocX=plocX+(x3-plocX)/smoothening\n",
    "            clocY=plocY+(y3-plocY)/smoothening\n",
    "            \n",
    "            pyautogui.moveTo(clocX,clocY)\n",
    "            plocX,plocY=clocX,clocY\n",
    "        \n",
    "        # Left click functionality\n",
    "        \n",
    "        if fingers[1]==1 and fingers[2]==1:\n",
    "            length,img,lineInfo=detector.findDistance(8,12,img)\n",
    "            # print(length)\n",
    "            if length<40:\n",
    "                pyautogui.click()\n",
    "        \n",
    "        # Design for right click\n",
    "        if fingers[1]==1 and fingers[2]==0:\n",
    "            if fingers[4]==1:\n",
    "                pyautogui.rightClick()\n",
    "                \n",
    "        \n",
    "    \n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime-pTime)\n",
    "    pTime = cTime\n",
    "\n",
    "    cv2.putText(img, f'FPS:{int(fps)}', (48, 70),\n",
    "                cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 3)\n",
    "    cv2.imshow(\"Image\",img)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fingers[\u001b[38;5;241m4\u001b[39m]:\n\u001b[0;32m     58\u001b[0m             volume\u001b[38;5;241m.\u001b[39mSetMasterVolumeLevelScalar(vol\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m---> 59\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mtoast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHello PythonðŸ\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m cTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     63\u001b[0m fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(cTime\u001b[38;5;241m-\u001b[39mpTime)\n",
      "File \u001b[1;32md:\\PROJECTS\\Hand Tracking\\.venv\\Lib\\site-packages\\win11toast.py:383\u001b[0m, in \u001b[0;36mtoast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoast_async\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\asyncio\\runners.py:186\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    187\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug\u001b[38;5;241m=\u001b[39mdebug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun(main)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# volume control gesture\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import HandTrackingModule as htm\n",
    "from win11toast import toast\n",
    "import math\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "wCam, hCam = 640, 480\n",
    "# boundingBox=[]\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, wCam)\n",
    "cap.set(4, hCam)\n",
    "pTime = 0\n",
    "\n",
    "\n",
    "detector = htm.HandDetector(maxHands=1)\n",
    "\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(\n",
    "    IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = interface.QueryInterface(IAudioEndpointVolume)\n",
    "volumeRange = volume.GetVolumeRange()\n",
    "minimumVolume = volumeRange[0]\n",
    "maximumVolume = volumeRange[1]\n",
    "area = 0\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img = cv2.flip(img, 1)  # Flipping is needed to create mirroring\n",
    "    # Find the hands\n",
    "    detector.findHands(img)\n",
    "    lmList, boundingBox = detector.findPosition(img, handNo=0, draw=True)\n",
    "    if len(lmList) != 0:\n",
    "        # Filter based on size\n",
    "        area = (boundingBox[2]-boundingBox[0]) * \\\n",
    "            (boundingBox[3]-boundingBox[1])//100\n",
    "        # print(area)\n",
    "        if 100 < area < 900:\n",
    "\n",
    "            # Find the distance btwn index and thumb\n",
    "            length, img, lineInfo = detector.findDistance(4, 8, img=img)\n",
    "\n",
    "            # Converting length to volume\n",
    "            vol = np.interp(length, [10, 150], [0, 100])\n",
    "\n",
    "            # Reduce resolution to make smoother.\n",
    "            smoothness = 5\n",
    "            vol = smoothness*round(vol/smoothness)\n",
    "\n",
    "            # Check fingers which are up\n",
    "            fingers=detector.fingersUp()\n",
    "          \n",
    "\n",
    "            # if pinky is down then set volume\n",
    "            if not fingers[4]:\n",
    "                volume.SetMasterVolumeLevelScalar(vol/100, None)\n",
    "                # await toast('Hello PythonðŸ')\n",
    "                # Add feedback to user with current volume range.\n",
    "            \n",
    "\n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime-pTime)\n",
    "    pTime = cTime\n",
    "\n",
    "    cv2.putText(img, f'FPS:{int(fps)}', (48, 70),\n",
    "                cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 3)\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
