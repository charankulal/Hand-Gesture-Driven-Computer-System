{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mouse actions\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import HandTrackingModule as htm\n",
    "import math\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "import pyautogui\n",
    "\n",
    "wCam, hCam = 640, 480\n",
    "cap=cv2.VideoCapture(0)\n",
    "cap.set(3,wCam)\n",
    "cap.set(4,hCam)\n",
    "pTime = 0\n",
    "frameR=100\n",
    "smoothening=12\n",
    "plocX,plocY=0,0\n",
    "clocX,clocY=0,0\n",
    "\n",
    "\n",
    "detector=htm.HandDetector(maxHands=1)\n",
    "wScreen,hScreen=pyautogui.size()\n",
    "pyautogui.FAILSAFE=False\n",
    "\n",
    "while True:\n",
    "    success,img=cap.read()\n",
    "    img = cv2.flip(img, 1)\n",
    "    img=detector.findHands(img)\n",
    "    lmList,bbox=detector.findPosition(img)\n",
    "    \n",
    "    if len(lmList)!=0:\n",
    "        [x1,y1]=lmList[8][1:]\n",
    "        [x2,y2]=lmList[12][1:]\n",
    "        fingers=detector.fingersUp()\n",
    "        cv2.rectangle(img,(frameR,frameR),(wCam-frameR,hCam-frameR),(255,0,255),2)\n",
    "        \n",
    "        # For hovering\n",
    "        if fingers[1]==1 and fingers[2]==0:\n",
    "            \n",
    "            x3=np.interp(x1,(frameR,wCam-frameR),(0,wScreen))\n",
    "            y3=np.interp(y1,(frameR,hCam-frameR),(0,hScreen))\n",
    "            \n",
    "            clocX=plocX+(x3-plocX)/smoothening\n",
    "            clocY=plocY+(y3-plocY)/smoothening\n",
    "            \n",
    "            pyautogui.moveTo(clocX,clocY)\n",
    "            plocX,plocY=clocX,clocY\n",
    "        \n",
    "        # Left click functionality\n",
    "        \n",
    "        if fingers[1]==1 and fingers[2]==1:\n",
    "            length,img,lineInfo=detector.findDistance(8,12,img)\n",
    "            # print(length)\n",
    "            if length<40:\n",
    "                pyautogui.click()\n",
    "        \n",
    "        # Design for right click\n",
    "        if fingers[1]==1 and fingers[2]==0:\n",
    "            if fingers[4]==1:\n",
    "                pyautogui.rightClick()\n",
    "                \n",
    "        \n",
    "    \n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime-pTime)\n",
    "    pTime = cTime\n",
    "\n",
    "    cv2.putText(img, f'FPS:{int(fps)}', (48, 70),\n",
    "                cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 3)\n",
    "    cv2.imshow(\"Image\",img)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volume control gesture --- Use right hand for the volume control\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import HandTrackingModule as htm\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "wCam, hCam = 640, 480\n",
    "# boundingBox=[]\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, wCam)\n",
    "cap.set(4, hCam)\n",
    "pTime = 0\n",
    "\n",
    "\n",
    "detector = htm.HandDetector(maxHands=1)\n",
    "\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(\n",
    "    IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = interface.QueryInterface(IAudioEndpointVolume)\n",
    "volumeRange = volume.GetVolumeRange()\n",
    "minimumVolume = volumeRange[0]\n",
    "maximumVolume = volumeRange[1]\n",
    "area = 0\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img = cv2.flip(img, 1)  # Flipping is needed to create mirroring\n",
    "    # Find the hands\n",
    "    img,which_hand=detector.findHands(img)\n",
    "    lmList, boundingBox = detector.findPosition(img, handNo=0, draw=True)\n",
    "    if which_hand==\"Right\":\n",
    "        if len(lmList) != 0:\n",
    "        # Filter based on size\n",
    "            area = (boundingBox[2]-boundingBox[0]) * \\\n",
    "                (boundingBox[3]-boundingBox[1])//100\n",
    "            # print(area)\n",
    "            if 100 < area < 900:\n",
    "\n",
    "                # Find the distance btwn index and thumb\n",
    "                length, img, lineInfo = detector.findDistance(4, 8, img=img)\n",
    "\n",
    "                # Converting length to volume\n",
    "                vol = np.interp(length, [10, 150], [0, 100])\n",
    "\n",
    "                # Reduce resolution to make smoother.\n",
    "                smoothness = 5\n",
    "                vol = smoothness*round(vol/smoothness)\n",
    "\n",
    "                # Check fingers which are up\n",
    "                fingers=detector.fingersUp()\n",
    "            \n",
    "\n",
    "                # if pinky is down then set volume\n",
    "                if not fingers[4]:\n",
    "                    volume.SetMasterVolumeLevelScalar(vol/100, None)\n",
    "                \n",
    "            \n",
    "\n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime-pTime)\n",
    "    pTime = cTime\n",
    "\n",
    "    cv2.putText(img, f'FPS:{int(fps)}', (48, 70),\n",
    "                cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 3)\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mflip(img, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flipping is needed to create mirroring\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Find the hands\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m img,which_hand\u001b[38;5;241m=\u001b[39m\u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindHands\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m lmList, boundingBox \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mfindPosition(img, handNo\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, draw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m which_hand\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLeft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\PROJECTS\\Hand Tracking\\HandTrackingModule.py:21\u001b[0m, in \u001b[0;36mHandDetector.findHands\u001b[1;34m(self, img, draw)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindHands\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, draw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     20\u001b[0m     imgRGB \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgRGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks:\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;66;03m# Both Hands are present in image(frame)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mmulti_handedness) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     25\u001b[0m             \u001b[38;5;66;03m# Display 'Both Hands' on the image\u001b[39;00m\n",
      "File \u001b[1;32md:\\PROJECTS\\Hand Tracking\\.venv\\Lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\PROJECTS\\Hand Tracking\\.venv\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:372\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    366\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    368\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    369\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    370\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import HandTrackingModule as htm\n",
    "import screen_brightness_control as sbc\n",
    "\n",
    "wCam, hCam = 640, 480\n",
    "# boundingBox=[]\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(3, wCam)\n",
    "cap.set(4, hCam)\n",
    "pTime = 0\n",
    "\n",
    "\n",
    "detector = htm.HandDetector(maxHands=1)\n",
    "area = 0\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img = cv2.flip(img, 1)  # Flipping is needed to create mirroring\n",
    "    # Find the hands\n",
    "    img,which_hand=detector.findHands(img)\n",
    "    lmList, boundingBox = detector.findPosition(img, handNo=0, draw=True)\n",
    "    if which_hand==\"Left\":\n",
    "        if len(lmList) != 0:\n",
    "        # Filter based on size\n",
    "            area = (boundingBox[2]-boundingBox[0]) * \\\n",
    "                (boundingBox[3]-boundingBox[1])//100\n",
    "            # print(area)\n",
    "            if 100 < area < 900:\n",
    "\n",
    "                # Find the distance btwn index and thumb\n",
    "                length, img, lineInfo = detector.findDistance(4, 8, img=img)\n",
    "\n",
    "                # Converting length to volume\n",
    "                brightness = np.interp(length, [10, 150], [0, 100])\n",
    "\n",
    "                # Reduce resolution to make smoother.\n",
    "                smoothness = 5\n",
    "                brightness = smoothness*round(brightness/smoothness)\n",
    "\n",
    "                # Check fingers which are up\n",
    "                fingers=detector.fingersUp()\n",
    "            \n",
    "\n",
    "                # if pinky is down then set volume\n",
    "                if not fingers[4]:\n",
    "                    sbc.set_brightness(brightness)\n",
    "\n",
    "    cTime = time.time()\n",
    "    fps = 1/(cTime-pTime)\n",
    "    pTime = cTime\n",
    "\n",
    "    cv2.putText(img, f'FPS:{int(fps)}', (48, 70),\n",
    "                cv2.FONT_HERSHEY_PLAIN, 2, (255, 0, 0), 3)\n",
    "    cv2.imshow(\"Image\", img)\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample use only..Not working\n",
    "\n",
    "# Importing Libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import numpy as np\n",
    "import HandTrackingModule as htm\n",
    "import math\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "# Used to convert protobuf message to a dictionary.\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "# Initializing the Model\n",
    "mp_holistic = mp.solutions.holistic\n",
    "holistic_model = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=1,\n",
    "    min_detection_confidence=0.75,\n",
    "    min_tracking_confidence=0.75,\n",
    "    max_num_hands=2)\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "# Start capturing video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Read video frame by frame\n",
    "    success, img = cap.read()\n",
    "\n",
    "    # Flip the image(frame)\n",
    "    img = cv2.flip(img, 1)\n",
    "\n",
    "    # Convert BGR image to RGB image\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the RGB image\n",
    "    results = hands.process(imgRGB)\n",
    "\n",
    "    # If hands are present in image(frame)\n",
    "    if results.multi_hand_landmarks:\n",
    "\n",
    "        # Both Hands are present in image(frame)\n",
    "        if len(results.multi_handedness) == 2:\n",
    "            # Display 'Both Hands' on the image\n",
    "            cv2.putText(img, 'Both Hands', (250, 50),\n",
    "                        cv2.FONT_HERSHEY_COMPLEX,\n",
    "                        0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # If any hand present\n",
    "        else:\n",
    "            for i in results.multi_handedness:\n",
    "\n",
    "                # Return whether it is Right or Left Hand\n",
    "                label = MessageToDict(i)['classification'][0]['label']\n",
    "\n",
    "                if label == 'Left':\n",
    "\n",
    "                    # Display 'Left Hand' on\n",
    "                    # left side of window\n",
    "                    cv2.putText(img, label+' Hand',\n",
    "                                (20, 50),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                0.9, (0, 255, 0), 2)\n",
    "\n",
    "                if label == 'Right':\n",
    "\n",
    "                    # Display 'Left Hand'\n",
    "                    # on left side of window\n",
    "                    cv2.putText(img, label+' Hand', (460, 50),\n",
    "                                cv2.FONT_HERSHEY_COMPLEX,\n",
    "                                0.9, (0, 255, 0), 2)\n",
    "                    # Draw landmark points\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "    # Display Video and when 'q'\n",
    "    # is entered, destroy the window\n",
    "    cv2.imshow('Image', img)\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
